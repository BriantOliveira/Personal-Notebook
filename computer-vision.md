**Edge Detection** - Identifying sharp changes in intensity in adjacent pixels.

**The canny edge detection technique**

The goal of edge detection is to identify the boundaries of objects within images.

In essence we'll be using a detection to try and find regions in an image where there is a sharp change in intensity a sharp change in color before diving into this.

It's important to recognize that an image can mirror it as a matrix an array of pixels a pixel contains

the light intensity at some location in the image.Each pixels intensity denoted by a numeric value that ranges from 0 to 255 and intensity value of zero

indicates no intensity.

```
[0 0 255 255]
[0 0 255 255]
[0 0 255 255]
[0 0 255 255]
```

If something is completely black Where s 255 represents maximum intensity something being completely. What's that being said gradient is that the change in brightness over a series of pixels. A strong gradient indicates a steep change whereas a small gradient represents a shallow change on the right hand side and you're looking at the gradient of the soccerball the outline of white pixels corresponds to the discontinuity in brightness at the points the strengthen gradient.

This helps us identify edges in our image since an edge is defined by the difference in intensity values in adjacent pixels.And wherever there is a sharp change in intensity a rapid change in brightness wherever there is a strong gradient there is a corresponding bright pixel in the gradient image by tracing out all of these pixels we obtain the edges.

We're going to use this intuition to detect the edges in our road image.This is a multi-step process.

**Step one** being to convert our image to grayscale. Why convert it to grayscale. Well as we discussed earlier images are made up of pixels. A three channel color image would have red green and blue channels each pixel a combination of three intensity values whereas a greyscale image only has one channel each pixel with only one intensity value ranging from 0 to 255.

The point being by using a grayscale image processing a single channel is faster than processing a three channel color image and less computational intensive. Let's start implementing this inside of them. We've already loaded and read our image into an array. Now what we'll do is import numb pie as the alias and P We're going to work with a copy of this array by setting a link image is equal to pie dog copy image.

Thus copying our array into a new variable it's imperative that you actually make a copy of the array instead of just setting lane image is equal to image. If we do this any changes we make to Lane image will also be reflected in the original viewable array.

Always ensure that you make a copy whenever working with a race instead of just setting them equal directly. So what we'll do now is we'll create a grayscale from the color image. We'll do that by setting a variable. Gray is equal to see to and from our open CV library will call the function CVT color which converts an image from one color space to another. We'll be converting lane image. And the second argument for an R G B to grayscale conversion. We can use the flag C-v to dot color underscore R G B to gray. Very intuitive.And now instead of showing the color image will show the gray image. If we go to our terminal Python Layne's that p y everything works out accordingly. This was step number one. Step number two of edge detection will be two.

**Gaussian Blur**

**Step two** is to now reduce noise and smooth in our image when detecting edges while it's important to accurately catch as many edges in the image as possible. We must filter out any image noise image noise can create false edges and ultimately affect edge detection. That's why it's imperative to filter it out and thus smoothen the image filtering out image noise and smoothing will be done with a Gaussian filter to understand the concept of a Gaussian filter. Recall that an image is stored as a collection of discrete pixels. Each of the pixels for a greyscale image is represented by a single number that describes the brightness of the pixel. For the sake of example how do we smooth in the following image.

_**The typical answer would be to modify the value of a pixel with the average value of the pixel intensities around it. Averaging out the pixels in the image to reduce noise will be done with the kernel. Essentially this kernel of normally distributed numbers is run across our entire image and sets each pixel about equal to the weighted average of its neighboring pixels thus smoothing our image**_. We're not going to go over kernel convolution and how it does it just know that when we write this line of code inside of our editor blur is equal to C-v to dog Gosden blur what we're doing is applying a gaussian blur on a greyscale image with a 5 by 5 kernel the size of the kernel is dependent on specific situations a 5 by 5 kernel is a good size for most cases.

But ultimately what that will do is returning a new image that we simply called blur. Applying the gaussian blur by involving our image with a kernel of Gaussian values reduces noise in our image back to our project set. Blur is equal to C-v to the gaussian blur. **We'll apply this blur in our gray scale image with our 5 by 5 kernel and we'll just leave the deviation is zero.**

The main thing you should take away from this is that we're using a gaussian blur to reduce noise in our greyscale scale image and now will simply show the blurred image. If we run this code Python Laine's dot p why there is our blurred greyscale image later on when we apply the Kenni method. It should be noted that this step was actually optional since the kidney function is going to internally apply a 5 by 5 Gaussian when we call it regardless. Now we know the theory with a 10 hour grayscale with smoothen data and reduced noise with a gaussian blur.

```
Strong Gradient        Small Gradient 
[0 0 255 255]            [0 0 20 20]
[0 0 255 255]            [0 0 20 20]
[0 0 255 255]            [0 0 20 20]
[0 0 255 255]            [0 0 20 20]
```

A small derivative is a small change in intensity whereas a big derivative is a big change by computing the derivative in all directions of the image. We're computing the gradients. Since recall the gradient is the change in brightness over a series of pixels. So when we call the kidney function it does all of that for us. It** computes the gradient in all directions of our blurred image CH and is then going to trace our strongest gradients as a series of white pixels**. **But notice these two arguments low threshold and high threshold. While this actually allows us to isolate the adjacent pixels that follow the strongest gradients if the gradient is larger than the upper threshold then it is accepted as an edge pixel.** If it is below the lower threshold it is rejected. If the gradient is between the thresholds then it will be accepted only if it is connected to a strong edge. The documentation itself recommends to use a ratio of 1 to 2 or 1 to 3 as such will use a low high threshold ratio of 1 to 3 50 to 150.

So far we've identified the edges in our image and isolated the region of interest. Now we'll make use of a technique that will detect straight lines in the image and thus identify the lane lines. This technique is known as hough transform will start by drawing a to d coordinates space of x and y and inside of it a straight line. We know that a straight line is represented by the equation Y is equal to x plus be nothing new so far. Just simple math our straight line has two parameters M and B. We're currently plotting it as a function of x and y but we can also represent this line in parametric space which we will call a space as B versus M. We now the y intercept of this line is 2. And the slope of the line is simply a rise over run. The change in y over the change in x which evaluates to three given the y intercept and slope this entire line can be plotted as a single point in Huff's space. Now imagine that instead of a line we had a single dot located at the coordinates 12 and 2. There are many possible lines that can pass through this dot each line with different values for M and B. You could have a line that crosses with M and be values of 2 and 8 3 and 6 4 and 4 5 and 2 6 and 0. So on and so forth. Notice that a single point in x and y space is represented by A-line and Huff's space. In other words by applauding the family of lines that goes through our point each line with its own distinct and B value pair. This produces an entire line of M in B value pairs enough space. What if we also had a point at in one. Once again there are many lines that can cross this point each line with different values for M and B. All of these different values for M and be represented by a line in parametric space. The point being whenever you see a series of points and we're told that these points are connected by some line. Ask yourself this question. What is that line. As previously mentioned there are many possible lines that can cross each point individually each line with different slope and wind or sub values. However there is one line that is consistent with both points. We can determine that by looking at the point of intersection enough space because that point of intersection represents the M and B values of a line consistent with crossing both of our points which in this case has slope and y intercept for Suppose there is one more point in our image space at the point sixteen and three. This point is also represented by A-line and parametric space.

Each point in that line the notes different values for it and B which once again correspond to different lines that can pass through this point but notice that there is another intersection at the same point which means that the line with the falling slope and y intercept four and four crosses all three of our dots. Why is this relevant. Well this idea of identifying possible lines from a series of points is how we're going to find lines in our gradient image. Recall that the gradient image is just a series of white points which represent edges in our image space. You and I can look at the very series of points in our image and automatically assume these points belong to a line. This series of points belongs to a line. So on and so forth. But what are the lines. What are their parameters. How do we identify them._** Well take these four points for example in our image space which correspond to the following Huff's space What we're going to do is first split our Huff's space into a grid. Each been inside of our grid corresponding to the slope and y intercept value of a candidate line.**_

For example what if I told you these points belong to a line. What is that line. Well I can see that there's points of intersection here. There are some here and some here as well. Well all of these points of intersection are inside of a single bin. For every point of intersection we're going to cast a vote inside of the bin that it belongs to the bin with the maximum number of votes that's going to be your line whatever and be value that this bin belongs to. That's the line that we're going to draw since it was voted as the line of best fit in describing our data. Now that we know the theory of how we're going to identify lines in our gradient image you would think to yourself are right enough talking time the code well not so fast. There's just one tiny problem. We still haven't taken into account vertical lines. Obviously if you try to compute the slope of a vertical line the change in x is zero. Which ultimately will always evaluate to a slope of infinity which is not something that we can represent and have space. Infinity is not really something we can work with anyway. We need a more robust representation of lines so that we don't encounter any numeric problems because clearly this form Y is equal to x plus be cannot represent vertical lines.

That being said instead of expressing our line with Cartesian coordinates system parameters M and B. Well instead express it in the polar coordinates system rho and theta such that our line equation can be read as RHO is equal to x Coast states plus y assigned data. If you're really interested in how this equation is derived feel free to ask me in the Q&A but the main idea is that this is still the equation of a line but in polar coordinates if I am to draw some line in Cartesian space the variable RHO is the perpendicular distance from the origin to that line. And data indicates the angle of inclination of the normal line from the X-axis which is measured in radians clockwise with respect to the positive x axis. Let's look at some examples. Suppose I had a point with exposition 5 and Y is equal to 2. And as you know this is a point and many lines can pass through this point including a vertical line.

_**We used to define lines passing through our points by their slope and y intercept and B but now they will be defined based on a row and data if we want to measure the perpendicular distance from the origin to the top of the line.**_ The angle of inclination of the normal line from the axis is simply 0 which works out to a distance of five. Another possible line that could pass through our point is a horizontal line the perpendicular distance from the origin to our line would correspond to an angle of 90 degrees which in radians is Pi over two.

Which ultimately works out to a distance of two. This line therefore is characterized by an angle theta of Pi over two and a distance a row of two just to strengthen our knowledge. Another possible line is the following whose perpendicular distance from origin to our line corresponds to an angle of 45 degrees from the positive axis that is Pi over 4 radians which works out to a distance a row of about 4.9 the point of all this being is that previously a point in image space represented a line in enough space. Whereas now with polar coordinates for a given point by plotting the family of lines that go through it each line or a distinct value for theta and row. We get a sinusoidal curve. This curve represents all of the different values for a row and data of lines that pass through our point.

This might look a bit intimidating but the concept is the exact same because imagine instead of one point we had 10 points which in turn results in 10 sinusoidal curse. As previously noted if the curves of different points intersect enough space and these points belong to the same line characterized by some row and data value. _**So this like before a line can be detected by finding the number of intersections between curves the more curves intersecting means that the line represented by that intersection crosses more points. In our case all ten of our curves intersect at a single point which means that there is a single line with some arrow and they value that crosses all ten of our dots. We can look at a more specific example with three dots in our Cartesian which represent the following sinusoidal curves.**_

All three lines intersect at the same points. Characterized by theta value of 0.9 to radians and perpendicular a distance of about nine point six secs. So that's the idea finding what slope best fits our data._** In our case it's the one with the following parameters.We can also apply the concept of voting that we discussed earlier such that our Huff's space is still in the form of a grid. And obviously this bin would have the maximum number of votes and just like before the bin and what the maximum number of votes that's going to be your line whatever theta in our valley that this bin belongs to.**_

That's the line that we draw. Since it was voted as the line of best fit in describing our data later on when we start implementing this we'll talk about the concept of thresholds. But for now that is all for transform. We're just trying to find the lines that best describe our points. And that's what we're going to use to find the lines that best find the etch points in our gradient.

---

#### **Intro to Neural Networks**

Machine learning is the concept of building computational systems that learn over time based on experience at its most basic level rather than explicitly programming hard coded instructions into a system machine learning uses a variety of algorithms to describe and analyze data learn from it improve and predict useful outcomes.

Often times it is not enough to directly program a computer to perform a specific task tasks such as driving a car. Speech recognition and object detection are way too complex to just program whereas the machine learning algorithm can learn and improve based on experience interact with its environment learn to detect and predict meaningful patterns to achieve desired results.The learning occurs between a learner and the environment. On that note it's important to distinguish between supervised and unsupervised learning. Supervised learning is the most popular machine learning technique but act as a guide to teach the algorithm what conclusions it should come up with.

Supervised learning typically begins with a data set that's associated with labeled features which define the meaning of our data and find patterns that can be applied to an analytics process. Another machine learning approach that occurs between the learner and its environment is unsupervised learning. In this case all the learner gets as training as large data sets with no labels and the learners task is to process that data find similarities and differences in the information provided and act on that information without prior training.

Let us first consider supervised learning. Consider the task of learning to classify handwritten digits in such a case the learner must figure out a rule for labeling a newly hand or in digit on that basis. _**A supervised learning algorithm with process images of hand or digits along with correct labels which denote the number. Each image represents what that will do is provide significant information that is missing in the newly inputted data which don't have a label to which we can apply the learning expertise and predict that missing information.**_

That is the learner is initially being supervised as it is provided with extra information in the beginning and then through convolutional neural networks learns the relationship between the hand ran images and the associated numbers and applies to that relationship to classify completely new images that the machine hasn't seen before. In essence whatever data set is presented to it it will try and predict its value based on prior training with previously labeled datasets. In this course we're mostly going to deal with supervised learning.

We mentioned that in supervised learning the learner is trained and makes use of data sets associated with labelled features which define the meaning of our data. That way when it's introduced to newly inputted data it's able to predict missing information based on established rule sets and patterns. In other words unsupervised learning a learner is provided examples of certain inputs and outputs and based on that training given a new input it's able to predict the corresponding outputs. In this lesson we'll talk about linear regression a supervised learning algorithm that allows us to make predictions based on linearly arranged datasets. For now we'll look into an example of a simple linear regression model which would model the relationship of a quantitative response variable and the explanatory variable the response variable is the dependent variable whose value we want to explain and predict based on the values of the independent variable. More specifically we'll look to establish a linear relationship between the price of the house with respect to its size the price of the house will be dependent on the size and as such it will represent our response variable whereas the size of the house is the independent variable whose value is we're going to use to try and make predictions about the price accordingly.

The size of the house is the inputs and the output is what we're trying to predict the price itself. We're currently given a set of data points which reflect the price of some houses based on their size and for each house with a given size. It's given a price label this small 400 square foot house is $210000. And the larger 600 square foot house is about $480000 What about the value of the 500 square foot house. Well we could just eyeball it and estimate that it's somewhere over here or even knew some kind of interpellation to get the exact value or we can draw the line that best fits our data. We're applying a linear function that best approximates the relationship between our two variables.This creates a predictive model for our data showing trends in our data and can thereby predict the values of new data which were not initially labeled. This is known as a linear regression.

Thanks to the linear function that we established from our previously labeled datasets. If we're given new inputs which don't have a label we're able to predict and estimate the output based on where the y of value falls on the regression line. That's one form of supervised learning predicting the values of new data based on a linear regression. And it's important to note that realistically data does not behave like a straight line. They are random x and y observations that on average may follow a linear pattern. Which is why we use linear regression models to represent them as such. The linear regression model also accounts for an error value.

Our predictive model through some of the learning rates learns to minimize this error by updating its linear function until no further improvement is possible. Thanks to this linear function that we established for my relabelled data sets one were given new inputs which don't have an output it's able to predict the most accurate value based on where its value falls on the regression line. Linear Regression is one of the most well-known and understood algorithms in machine learning. This actually segues into our next discussion where we'll also learn how we can use lines to classify data among discrete classes. This is another form of supervised learning called classification. A very important concept in self-driving cars.

**Classification**

Now we'll cover another form of supervised learning which will encounter all throughout the course and that is classification a very important concept in self-driving cars classification predictive modeling approximates a mapping function which predicts the class or category for a given observation in the most simple terms. It takes some kind of input X and classifies it maps it to some category generally based on previous data sets. It would calculate the likelihood of something belonging to some class. The best way to get a broad sense of how classification works is to go through an example say we're a hospital which annually tests thousands of patients. It's our job based on that person's condition to determine whether or not they are likely to become diabetic. Let's suppose the examination contains information regarding two attributes the person's blood sugar and their H. Generally speaking high blood sugar normally indicates a resistance to insulin without insulin. A lot of sugar builds up in the bloodstream leading to diabetes. As far as age goes if someone gets older generally speaking they tend to exercise less gain weight. Increasing the risk of diabetes as well. Obviously in diagnosis a lot of other factors are considered like blood pressure cholesterol levels family history etc. but for the sake of our model and keeping things simple We'll assume that the higher your blood sugar and the older you are the more likely you are to become diabetic suppose one person has a blood sugar levels of two million miles per liter and is 20 years old. This person is young and their blood sugar levels are relatively low and thus unlikely that they would be diabetic. They're in good health. Another person is 60 years old with blood sugar levels of 11 million miles per liter. Obviously this person is relatively older with a very high blood sugar. So don't assume it's very likely that they are diabetic. Finally there is another person who is 45 years old with blood sugar levels of five miles per liter. Is this person likely to have the disease or not likely. This one's a bit of a question mark. We need some kind of predictive model which can predict the category the class to which a person belongs on based on this given input.

The category as being is that person likely to have the disease or not. Well start by representing this predictive model inside of a two dimensional Cartesian coordinates system blood glucose levels will traverse the vertical axis which will range from zero to 12 million miles per liter. And age will traverse the horizontal axis. In our case simply ranging from zero to 100 years what we'll do now is plot the people we have so far. The first one will be plaudit on the coordinates that the note to my Lemos per liter of sugar and 20 years old.

Located in the bottom left of our record in space the second person will be plaudit on the coordinates denoting 11 miles per liter and 60 years old. Let's assume that we ended up testing both of our patients and actually found out that this person indeed did have diabetes and this person did not. Thus we can assign each one a label such that this applicant has the disease and this one does not will denote not being diabetic as blue and actually having the diseases red. How does this determine whether or not the person who's 45 years old with five million miles per liter of glucose level have diabetes or not to which class do we classify this person is he likely to have the disease or not.

Recall that since classification is a supervised form of learning we need previously labeled data sets so that we can develop a model which is going to learn how to calculate the likelihood of a new patient to belonging to some class. What you're seeing now all of these points are actually patients so we ended up testing.We found out which ones were diabetic and which ones weren't. Anyone that didn't have it was labeled as blue. And anyone that did carry the disease is labeled a red. Clearly there's a pattern in our data. The higher your age and blood sugar the more likely you are to become diabetic. What about this person who's 45 years old with sugar levels of five miles per liter. This person unlike the others we haven't actually tested them.

They have not been labeled. But based on previous data it can be confidently predicted that no this person is not likely to have the disease. You can easily look and see that previous patients with similar blood sugar an age whom we actually tested are not diabetic. Thus we can conclude that this person is likely not to be diabetic either. Now recall we previously discussed how we can use lines to classify data among discrete classes well simply enough our data can be separated by a line.

This line is represented by some linear equation. How do we obtain the equation of this line. Well that's a much longer discussion and that we'll leave for later. But essentially there is a clear distinction that patients above the line are diabetic and patients below the line are not. That being said based on this line based on our predictive model which was obtained thanks to previous data sets we can effectively predict the probability of a new patient having diabetes. If for example someone has blood sugar levels or formula or supreme leader and is 55 years old this is a new datasets and it's not labeled. We haven't actually tested this person so we don't know.

We need to use our linear model which we obtained from previously labeled data sets to make a prediction. Clearly this co-ordinate is Plot and below the line. It can then be confidently assumed that this person is not likely to be diabetic. In summary what we're doing is first looking at previous data sets of patients who have already been labeled as having the disease or not. And based on that data we can come up with a pattern a predictive linear model a line that best separates our data into discrete classes classification in itself is a very important concept in self-driving cars. Implementing an algorithm is able to find this line with minimal error will be the entire focus of the section solving classification problems is essential in self-driving cars and can be used to classify whether an object crossing the road is a car a pedestrian a bicycle or even identifying different traffic signs.

This line is found based on previous datasets that have already been labeled as either healthy or sick this sets up a predictive model that can help us predict the labels of new unobserved data. In our case we developed a model that predicts the person's likeliness of being diabetic. And from that model depending on that person's age and blood sugar if the coordinate line is above the line then they're likely diabetic otherwise not. Let us now approach this a little bit more analytically to make a prediction.

First two inputs are processed. _**The first one is age which will be denoted as x 1 and the second one is the blood sugar which will be denoted as x 2. Accordingly the horizontal axis corresponds to the variable x 1 and you're probably used to the vertical axis being labeled as a y. We're going to label it x 2 so as to stay consistent and properly denote the second input variable.**_ Right now you and I can eyeball this data and dry a boundary line that separates the two regions considerably well eventually we'll implement an algorithm that's able to find this line on its own with a minimal error. Well let's assume that there are hundreds of iterations we're learning system that Terman that this line separates I read it into discrete classes. We can now use this data to classify new data as either healthy or sick depending on whether or not the data point falls below the line or above it the slides equation can be represented as x 2 is equal to negative 0.05 X1 plus 12. You're probably used to representing the equation of a line is y is equal to XP. Well this is the same exact thing as why is equal TMX supposed to be. That is where a Y would equal the 0.05 x 12. But it's important that we follow this notation instead so as to properly reference our input variables. What we'll do now is actually rearrange of this linear equation by bringing X to over to the other side whereas 0 = 0.05 X1 - x 2 + 12.

This is still the equation of our reline just to rearrange it. And the reason why we did this will be very clear momentarily. Let's take this equation and if you take any point that's above the line substitute to the linear equation where in this case X one is 80 and x 2 is 10. So being this and our the equation we get a negative number. Similarly if you take any point that's below the line we're in this case x 1 is 20 and x 2 was 5 7 into the equation and notice that you get a Howzat of number this linear model which we were able to develop based on previous data sets. Its equation can be used to determine the score if the person scores positive such that it's greater than or equal to zero. They likely are not diabetic. Otherwise if the person's score is negative they likely are diabetic.

Thus separating our data into two discrete classes. Let's recall a distinction between the label and the prediction. These data sets they've already been labeled as either red or blue. They have been given a correct output as having diabetes or not. The goal is to use these labeled datasets to train an algorithm to develop a linear model that can accurately classify this data. Initially the algorithms start with the random line with some random equation. Notice that X-1 and x 2 are multiplied by two constants. These two constants are called the weights which essentially dictate the slope of the line. And this year is known as the bias whose value dictates where the line intersects the vertical axis different weights and biases will result in different slopes and inclinations of our align as clearly shown in generally the equation for a linear model will be written as one X1 plus W 2 looks looks plus be what our algorithm will do is look at the initial data that's already been labeled. It will start with some random line to try and classify our training data alone like this one. Clearly this linear model is not a good one it misclassified a lot of the points its classification accuracy is not good and thus its error is very large. What our algorithm will do is it will keep adjusting the weights and biases of our linear model. Until it comes up with the optimized line that best classifies our data the line with minimal error our algorithm just made use of global datasets to train itself and come up with the perfect linear model. Let's assume this linear model has the following equation. We can now confidently use this model to predict the label of new data points. We do so by first determining the score. Suppose we have a person who has aged 40 years old with a blood sugar of two will then take the linear combination of these inputs which results in a score of eights. Now we can make a prediction. The prediction is noted as y hat which says that if that person with some age and some blood sugar if their score.

If the linear combination of their inputs is greater than or equal to zero if they are positive they are classified in Region 1. Otherwise if their score their linear combination is less than zero if it's negative they will be classified in region zero. In our case the score of a person aged 40 with the glucose levels of tumor the most supreme leader has a positive score and thus they can be categorized and costs one as not being diabetic which makes sense in the case of this line's equation since anything above the line will have a negative for anything below a positive score. The entire focus of the suction will be to train an algorithm to come up with a line whose predictions are accurate with data points that have already been labeled. That ends it must be able to correctly classify all the data points with maximum accuracy. There is one more concept we have to understand before we implement this into code and that is the concept of perceptrons.

The brain is a complex information processing device with incredible pattern recognition abilities it's able to freely process input from its surrounding environment categorize them and generate some kind of output. Similarly the perceptron is a basic processing element that also receives input processes it and generates an output based on the predictive model. More specifically those predictive linear model which was developed based on previously labeled data sets.

Let's place it inside of a node our model node is going to receive two input nodes the age and blood sugar x1 and x2 respectively. Suppose we want to predict if someone is diabetic the person is 30 years old has a blood sugar levels of three multi-month per liter. As such the input note X1 is 30 years old and the input node X2 is three the most per liter assume the line is the following equation.

The Perceptron on woman plot the point on the graph and check where that point is relative to the linear model based on this Lund's equation. If the person's linear combination is positive there are going to be plotted below the line and thus put a prediction of 1. Otherwise if the person's linear combination is negative they will be classified above the line and thus output a prediction of zero. In our case this person who is 30 years old with 3 million miles per liter of blood sugar has a positive linear combination is therefore going to be plotted somewhere below the line along with the other Blue

Points in Region 1 and thus we can safely predict that this person is in Class 1. They are not diabetic. Let's reshape our perception model and consider the bias as an input as well meaning we'll consider the person's age and the blood sugar and a bias value of one y one. Well if you really think about it what the first node in the perception does is it takes the inputs and multiplies each of them by their respective weights. The age X1 is multiplied by W 1 negative 0.1. The blood sugar x 2 is multiplied by W2 which would simply be one and the bias one is simply multiplied by the Bias value 14 and then taking the sum to obtain the score. There is actually a second node in the perceptron which we haven't talked about but we've been subconsciously using it all along and that is the activation function in neural networks the activation function of a node is what defines the outputs.

The most common type of activation function that we've been subconsciously using all along is the step function we've established that what the first node does is it takes the linear combination of all these inputs and now it's the step function which checks the result of our linear combination. If it's positive the point is below the line and therefore assign a label of 1. Otherwise if the score is less than zero The point is above the line and assign the label of zero. To recap all of this our input nodes whose linear combination is computed in the model node whose results then processed into the activation function which then predicts the outputs. All of this makes up our perception now that we know the basic node composition of a perception.

![](/assets/Screen Shot 2019-09-24 at 5.34.57 PM.png)The step function then checks the score and assigns that point a label of either 0 or 1. That is the basic composition of a perceptual and it takes an input data process that information in some way and finally the science how to categorize that data by producing and outputs. We also mentioned that when taking the linear combination of the inputs each input is multiplied by its respective weights. Looking at this equation age X1 is multiplied by a weight one and sugar level x 2 is multiplied by weight too.

Looking at this it should be clear that blood sugar has more weight than H. Suppose someone is 50 years old and has three million miles per liter of blood sugar. If we take their linear combination it results in a positive value of Six which our activation function would process and return a prediction of 1. If I change the age by a factor of 10 make it 60 years old it barely changes the value. Whereas if I change the sugar level by a factor of 10 and it has a dramatic impact on the score the input with the highest weight is the one that has the larger effect on the score. And clearly blood sugar has the higher effects its value is much more influential and pretty much dictates whether or not someone falls in class 1 or class 0 whereas age doesn't really have as much of an effect.



The goal is to use this data to train and develop an algorithm to come up with a line that best separates data into discrete classes with minimal error. As you know a line can be represented by an equation where w 1 and W to the weights dictate the slope of the line. These weights start out as the random values. And so we're just going to have a random line little more often than not is not going to classify our data correctly. Clearly this one does not. But as the neural network learns more about what kind of input did its dealing with it will adjust the weights based on the output errors that resulted in categorizing the data with the previous waits until it comes up with a good model. In essence through every iteration the network is being trained to come up with a better linear model.

And how does it do it. Pacifically How does it know how to adjust the weights instead of looking at 200 points. Let's simplify our example to 8 points the Blue Points are labeled 1 and the red points are also already labeled as 0. Now we need a line that separates this data into two classes. You and I can eyeball this and draw the following line that separates all of our data correctly. But the computer can eyeball it. It has no idea how to start. So what it's going to do is display a random line with some random equation. It's then going to look at this line and identify any errors in categorizing the falling datasets based on these errors. It's going to eventually adjust the weights of the linear your equation to fit the data better.

Where I essentially training the neural network. But how is it going to do that. Let's go back to our first line. Clearly these two points are misclassified. Suppose this is really in your equation. If you take the linear combination of each point each one would result in a negative score a value of less than zero. And I would step function would then predict each one the value is zero. But this is wrong since we initially assigned these points a label of 1. But our linear model predicts their value is zero. The key metric to solving this problem is the error which tells us if our model is doing well or not depending on how large or how small the error is.

And now through a gradient since our network is going to take tiny steps to reduce this error in every iteration these tiny steps correspond to the learning rate which needs to be sufficiently small since when that line is fixing itself you never want it to move drastically in one direction especially when you have a lot of points at every step. The line will move closer to them misclassified points and it keeps doing so until there are no errors or until the error is sufficiently small. And by doing so we eventually obtain the perfect linear model. Suppose the perfect linear model for the set of data was defined by the following equation. We cannot conclude that in classifying our data our linear model clearly gives more weight to X to rather the next one. The main theme of this lesson being the algorithm starts with a random line defined by random weights. The neural network as it calculates the errors it starts to learn more about the input data and then readjust the weights to minimize these errors and thus better classify our data. How does it calculate the air. How do we determine which points have the largest error relative to the current linear model.

