**Edge Detection** - Identifying sharp changes in intensity in adjacent pixels.

**The canny edge detection technique**

The goal of edge detection is to identify the boundaries of objects within images.

In essence we'll be using a detection to try and find regions in an image where there is a sharp change in intensity a sharp change in color before diving into this.

It's important to recognize that an image can mirror it as a matrix an array of pixels a pixel contains

the light intensity at some location in the image.Each pixels intensity denoted by a numeric value that ranges from 0 to 255 and intensity value of zero

indicates no intensity.

```
[0 0 255 255]
[0 0 255 255]
[0 0 255 255]
[0 0 255 255]
```

If something is completely black Where s 255 represents maximum intensity something being completely. What's that being said gradient is that the change in brightness over a series of pixels. A strong gradient indicates a steep change whereas a small gradient represents a shallow change on the right hand side and you're looking at the gradient of the soccerball the outline of white pixels corresponds to the discontinuity in brightness at the points the strengthen gradient.

This helps us identify edges in our image since an edge is defined by the difference in intensity values in adjacent pixels.And wherever there is a sharp change in intensity a rapid change in brightness wherever there is a strong gradient there is a corresponding bright pixel in the gradient image by tracing out all of these pixels we obtain the edges.

We're going to use this intuition to detect the edges in our road image.This is a multi-step process.

**Step one** being to convert our image to grayscale. Why convert it to grayscale. Well as we discussed earlier images are made up of pixels. A three channel color image would have red green and blue channels each pixel a combination of three intensity values whereas a greyscale image only has one channel each pixel with only one intensity value ranging from 0 to 255.

The point being by using a grayscale image processing a single channel is faster than processing a three channel color image and less computational intensive. Let's start implementing this inside of them. We've already loaded and read our image into an array. Now what we'll do is import numb pie as the alias and P We're going to work with a copy of this array by setting a link image is equal to pie dog copy image.

Thus copying our array into a new variable it's imperative that you actually make a copy of the array instead of just setting lane image is equal to image. If we do this any changes we make to Lane image will also be reflected in the original viewable array.

Always ensure that you make a copy whenever working with a race instead of just setting them equal directly. So what we'll do now is we'll create a grayscale from the color image. We'll do that by setting a variable. Gray is equal to see to and from our open CV library will call the function CVT color which converts an image from one color space to another. We'll be converting lane image. And the second argument for an R G B to grayscale conversion. We can use the flag C-v to dot color underscore R G B to gray. Very intuitive.And now instead of showing the color image will show the gray image. If we go to our terminal Python Layne's that p y everything works out accordingly. This was step number one. Step number two of edge detection will be two.

**Gaussian Blur**

**Step two** is to now reduce noise and smooth in our image when detecting edges while it's important to accurately catch as many edges in the image as possible. We must filter out any image noise image noise can create false edges and ultimately affect edge detection. That's why it's imperative to filter it out and thus smoothen the image filtering out image noise and smoothing will be done with a Gaussian filter to understand the concept of a Gaussian filter. Recall that an image is stored as a collection of discrete pixels. Each of the pixels for a greyscale image is represented by a single number that describes the brightness of the pixel. For the sake of example how do we smooth in the following image.

_**The typical answer would be to modify the value of a pixel with the average value of the pixel intensities around it. Averaging out the pixels in the image to reduce noise will be done with the kernel. Essentially this kernel of normally distributed numbers is run across our entire image and sets each pixel about equal to the weighted average of its neighboring pixels thus smoothing our image**_. We're not going to go over kernel convolution and how it does it just know that when we write this line of code inside of our editor blur is equal to C-v to dog Gosden blur what we're doing is applying a gaussian blur on a greyscale image with a 5 by 5 kernel the size of the kernel is dependent on specific situations a 5 by 5 kernel is a good size for most cases.

But ultimately what that will do is returning a new image that we simply called blur. Applying the gaussian blur by involving our image with a kernel of Gaussian values reduces noise in our image back to our project set. Blur is equal to C-v to the gaussian blur. **We'll apply this blur in our gray scale image with our 5 by 5 kernel and we'll just leave the deviation is zero.**

The main thing you should take away from this is that we're using a gaussian blur to reduce noise in our greyscale scale image and now will simply show the blurred image. If we run this code Python Laine's dot p why there is our blurred greyscale image later on when we apply the Kenni method. It should be noted that this step was actually optional since the kidney function is going to internally apply a 5 by 5 Gaussian when we call it regardless. Now we know the theory with a 10 hour grayscale with smoothen data and reduced noise with a gaussian blur.

```
Strong Gradient        Small Gradient 
[0 0 255 255]            [0 0 20 20]
[0 0 255 255]            [0 0 20 20]
[0 0 255 255]            [0 0 20 20]
[0 0 255 255]            [0 0 20 20]
```

A small derivative is a small change in intensity whereas a big derivative is a big change by computing the derivative in all directions of the image. We're computing the gradients. Since recall the gradient is the change in brightness over a series of pixels. So when we call the kidney function it does all of that for us. It** computes the gradient in all directions of our blurred image CH and is then going to trace our strongest gradients as a series of white pixels**. **But notice these two arguments low threshold and high threshold. While this actually allows us to isolate the adjacent pixels that follow the strongest gradients if the gradient is larger than the upper threshold then it is accepted as an edge pixel.** If it is below the lower threshold it is rejected. If the gradient is between the thresholds then it will be accepted only if it is connected to a strong edge. The documentation itself recommends to use a ratio of 1 to 2 or 1 to 3 as such will use a low high threshold ratio of 1 to 3 50 to 150.

So far we've identified the edges in our image and isolated the region of interest. Now we'll make use of a technique that will detect straight lines in the image and thus identify the lane lines. This technique is known as hough transform will start by drawing a to d coordinates space of x and y and inside of it a straight line. We know that a straight line is represented by the equation Y is equal to x plus be nothing new so far. Just simple math our straight line has two parameters M and B. We're currently plotting it as a function of x and y but we can also represent this line in parametric space which we will call a space as B versus M. We now the y intercept of this line is 2. And the slope of the line is simply a rise over run. The change in y over the change in x which evaluates to three given the y intercept and slope this entire line can be plotted as a single point in Huff's space. Now imagine that instead of a line we had a single dot located at the coordinates 12 and 2. There are many possible lines that can pass through this dot each line with different values for M and B. You could have a line that crosses with M and be values of 2 and 8 3 and 6 4 and 4 5 and 2 6 and 0. So on and so forth. Notice that a single point in x and y space is represented by A-line and Huff's space. In other words by applauding the family of lines that goes through our point each line with its own distinct and B value pair. This produces an entire line of M in B value pairs enough space. What if we also had a point at in one. Once again there are many lines that can cross this point each line with different values for M and B. All of these different values for M and be represented by a line in parametric space. The point being whenever you see a series of points and we're told that these points are connected by some line. Ask yourself this question. What is that line. As previously mentioned there are many possible lines that can cross each point individually each line with different slope and wind or sub values. However there is one line that is consistent with both points. We can determine that by looking at the point of intersection enough space because that point of intersection represents the M and B values of a line consistent with crossing both of our points which in this case has slope and y intercept for Suppose there is one more point in our image space at the point sixteen and three. This point is also represented by A-line and parametric space.

Each point in that line the notes different values for it and B which once again correspond to different lines that can pass through this point but notice that there is another intersection at the same point which means that the line with the falling slope and y intercept four and four crosses all three of our dots. Why is this relevant. Well this idea of identifying possible lines from a series of points is how we're going to find lines in our gradient image. Recall that the gradient image is just a series of white points which represent edges in our image space. You and I can look at the very series of points in our image and automatically assume these points belong to a line. This series of points belongs to a line. So on and so forth. But what are the lines. What are their parameters. How do we identify them._** Well take these four points for example in our image space which correspond to the following Huff's space What we're going to do is first split our Huff's space into a grid. Each been inside of our grid corresponding to the slope and y intercept value of a candidate line.**_

For example what if I told you these points belong to a line. What is that line. Well I can see that there's points of intersection here. There are some here and some here as well. Well all of these points of intersection are inside of a single bin. For every point of intersection we're going to cast a vote inside of the bin that it belongs to the bin with the maximum number of votes that's going to be your line whatever and be value that this bin belongs to. That's the line that we're going to draw since it was voted as the line of best fit in describing our data. Now that we know the theory of how we're going to identify lines in our gradient image you would think to yourself are right enough talking time the code well not so fast. There's just one tiny problem. We still haven't taken into account vertical lines. Obviously if you try to compute the slope of a vertical line the change in x is zero. Which ultimately will always evaluate to a slope of infinity which is not something that we can represent and have space. Infinity is not really something we can work with anyway. We need a more robust representation of lines so that we don't encounter any numeric problems because clearly this form Y is equal to x plus be cannot represent vertical lines.

That being said instead of expressing our line with Cartesian coordinates system parameters M and B. Well instead express it in the polar coordinates system rho and theta such that our line equation can be read as RHO is equal to x Coast states plus y assigned data. If you're really interested in how this equation is derived feel free to ask me in the Q&A but the main idea is that this is still the equation of a line but in polar coordinates if I am to draw some line in Cartesian space the variable RHO is the perpendicular distance from the origin to that line. And data indicates the angle of inclination of the normal line from the X-axis which is measured in radians clockwise with respect to the positive x axis. Let's look at some examples. Suppose I had a point with exposition 5 and Y is equal to 2. And as you know this is a point and many lines can pass through this point including a vertical line.

_**We used to define lines passing through our points by their slope and y intercept and B but now they will be defined based on a row and data if we want to measure the perpendicular distance from the origin to the top of the line.**_ The angle of inclination of the normal line from the axis is simply 0 which works out to a distance of five. Another possible line that could pass through our point is a horizontal line the perpendicular distance from the origin to our line would correspond to an angle of 90 degrees which in radians is Pi over two.

Which ultimately works out to a distance of two. This line therefore is characterized by an angle theta of Pi over two and a distance a row of two just to strengthen our knowledge. Another possible line is the following whose perpendicular distance from origin to our line corresponds to an angle of 45 degrees from the positive axis that is Pi over 4 radians which works out to a distance a row of about 4.9 the point of all this being is that previously a point in image space represented a line in enough space. Whereas now with polar coordinates for a given point by plotting the family of lines that go through it each line or a distinct value for theta and row. We get a sinusoidal curve. This curve represents all of the different values for a row and data of lines that pass through our point.

This might look a bit intimidating but the concept is the exact same because imagine instead of one point we had 10 points which in turn results in 10 sinusoidal curse. As previously noted if the curves of different points intersect enough space and these points belong to the same line characterized by some row and data value. _**So this like before a line can be detected by finding the number of intersections between curves the more curves intersecting means that the line represented by that intersection crosses more points. In our case all ten of our curves intersect at a single point which means that there is a single line with some arrow and they value that crosses all ten of our dots. We can look at a more specific example with three dots in our Cartesian which represent the following sinusoidal curves.**_

All three lines intersect at the same points. Characterized by theta value of 0.9 to radians and perpendicular a distance of about nine point six secs. So that's the idea finding what slope best fits our data._** In our case it's the one with the following parameters.We can also apply the concept of voting that we discussed earlier such that our Huff's space is still in the form of a grid. And obviously this bin would have the maximum number of votes and just like before the bin and what the maximum number of votes that's going to be your line whatever theta in our valley that this bin belongs to.**_

That's the line that we draw. Since it was voted as the line of best fit in describing our data later on when we start implementing this we'll talk about the concept of thresholds. But for now that is all for transform. We're just trying to find the lines that best describe our points. And that's what we're going to use to find the lines that best find the etch points in our gradient.

---

#### **Intro to Neural Networks**

Machine learning is the concept of building computational systems that learn over time based on experience at its most basic level rather than explicitly programming hard coded instructions into a system machine learning uses a variety of algorithms to describe and analyze data learn from it improve and predict useful outcomes.

Often times it is not enough to directly program a computer to perform a specific task tasks such as driving a car. Speech recognition and object detection are way too complex to just program whereas the machine learning algorithm can learn and improve based on experience interact with its environment learn to detect and predict meaningful patterns to achieve desired results.The learning occurs between a learner and the environment. On that note it's important to distinguish between supervised and unsupervised learning. Supervised learning is the most popular machine learning technique but act as a guide to teach the algorithm what conclusions it should come up with.

Supervised learning typically begins with a data set that's associated with labeled features which define the meaning of our data and find patterns that can be applied to an analytics process. Another machine learning approach that occurs between the learner and its environment is unsupervised learning. In this case all the learner gets as training as large data sets with no labels and the learners task is to process that data find similarities and differences in the information provided and act on that information without prior training.

Let us first consider supervised learning. Consider the task of learning to classify handwritten digits in such a case the learner must figure out a rule for labeling a newly hand or in digit on that basis. _**A supervised learning algorithm with process images of hand or digits along with correct labels which denote the number. Each image represents what that will do is provide significant information that is missing in the newly inputted data which don't have a label to which we can apply the learning expertise and predict that missing information.**_

That is the learner is initially being supervised as it is provided with extra information in the beginning and then through convolutional neural networks learns the relationship between the hand ran images and the associated numbers and applies to that relationship to classify completely new images that the machine hasn't seen before. In essence whatever data set is presented to it it will try and predict its value based on prior training with previously labeled datasets. In this course we're mostly going to deal with supervised learning.

We mentioned that in supervised learning the learner is trained and makes use of data sets associated with labelled features which define the meaning of our data. That way when it's introduced to newly inputted data it's able to predict missing information based on established rule sets and patterns. In other words unsupervised learning a learner is provided examples of certain inputs and outputs and based on that training given a new input it's able to predict the corresponding outputs. In this lesson we'll talk about linear regression a supervised learning algorithm that allows us to make predictions based on linearly arranged datasets. For now we'll look into an example of a simple linear regression model which would model the relationship of a quantitative response variable and the explanatory variable the response variable is the dependent variable whose value we want to explain and predict based on the values of the independent variable. More specifically we'll look to establish a linear relationship between the price of the house with respect to its size the price of the house will be dependent on the size and as such it will represent our response variable whereas the size of the house is the independent variable whose value is we're going to use to try and make predictions about the price accordingly.

The size of the house is the inputs and the output is what we're trying to predict the price itself. We're currently given a set of data points which reflect the price of some houses based on their size and for each house with a given size. It's given a price label this small 400 square foot house is $210000. And the larger 600 square foot house is about $480000 What about the value of the 500 square foot house. Well we could just eyeball it and estimate that it's somewhere over here or even knew some kind of interpellation to get the exact value or we can draw the line that best fits our data. We're applying a linear function that best approximates the relationship between our two variables.This creates a predictive model for our data showing trends in our data and can thereby predict the values of new data which were not initially labeled. This is known as a linear regression.

Thanks to the linear function that we established from our previously labeled datasets. If we're given new inputs which don't have a label we're able to predict and estimate the output based on where the y of value falls on the regression line. That's one form of supervised learning predicting the values of new data based on a linear regression. And it's important to note that realistically data does not behave like a straight line. They are random x and y observations that on average may follow a linear pattern. Which is why we use linear regression models to represent them as such. The linear regression model also accounts for an error value.

Our predictive model through some of the learning rates learns to minimize this error by updating its linear function until no further improvement is possible. Thanks to this linear function that we established for my relabelled data sets one were given new inputs which don't have an output it's able to predict the most accurate value based on where its value falls on the regression line. Linear Regression is one of the most well-known and understood algorithms in machine learning. This actually segues into our next discussion where we'll also learn how we can use lines to classify data among discrete classes. This is another form of supervised learning called classification. A very important concept in self-driving cars.

**Classification**

Now we'll cover another form of supervised learning which will encounter all throughout the course and that is classification a very important concept in self-driving cars classification predictive modeling approximates a mapping function which predicts the class or category for a given observation in the most simple terms. It takes some kind of input X and classifies it maps it to some category generally based on previous data sets. It would calculate the likelihood of something belonging to some class. The best way to get a broad sense of how classification works is to go through an example say we're a hospital which annually tests thousands of patients. It's our job based on that person's condition to determine whether or not they are likely to become diabetic. Let's suppose the examination contains information regarding two attributes the person's blood sugar and their H. Generally speaking high blood sugar normally indicates a resistance to insulin without insulin. A lot of sugar builds up in the bloodstream leading to diabetes. As far as age goes if someone gets older generally speaking they tend to exercise less gain weight. Increasing the risk of diabetes as well. Obviously in diagnosis a lot of other factors are considered like blood pressure cholesterol levels family history etc. but for the sake of our model and keeping things simple We'll assume that the higher your blood sugar and the older you are the more likely you are to become diabetic suppose one person has a blood sugar levels of two million miles per liter and is 20 years old. This person is young and their blood sugar levels are relatively low and thus unlikely that they would be diabetic. They're in good health. Another person is 60 years old with blood sugar levels of 11 million miles per liter. Obviously this person is relatively older with a very high blood sugar. So don't assume it's very likely that they are diabetic. Finally there is another person who is 45 years old with blood sugar levels of five miles per liter. Is this person likely to have the disease or not likely. This one's a bit of a question mark. We need some kind of predictive model which can predict the category the class to which a person belongs on based on this given input.

The category as being is that person likely to have the disease or not. Well start by representing this predictive model inside of a two dimensional Cartesian coordinates system blood glucose levels will traverse the vertical axis which will range from zero to 12 million miles per liter. And age will traverse the horizontal axis. In our case simply ranging from zero to 100 years what we'll do now is plot the people we have so far. The first one will be plaudit on the coordinates that the note to my Lemos per liter of sugar and 20 years old.

Located in the bottom left of our record in space the second person will be plaudit on the coordinates denoting 11 miles per liter and 60 years old. Let's assume that we ended up testing both of our patients and actually found out that this person indeed did have diabetes and this person did not. Thus we can assign each one a label such that this applicant has the disease and this one does not will denote not being diabetic as blue and actually having the diseases red. How does this determine whether or not the person who's 45 years old with five million miles per liter of glucose level have diabetes or not to which class do we classify this person is he likely to have the disease or not.

Recall that since classification is a supervised form of learning we need previously labeled data sets so that we can develop a model which is going to learn how to calculate the likelihood of a new patient to belonging to some class. What you're seeing now all of these points are actually patients so we ended up testing.We found out which ones were diabetic and which ones weren't. Anyone that didn't have it was labeled as blue. And anyone that did carry the disease is labeled a red. Clearly there's a pattern in our data. The higher your age and blood sugar the more likely you are to become diabetic. What about this person who's 45 years old with sugar levels of five miles per liter. This person unlike the others we haven't actually tested them.

They have not been labeled. But based on previous data it can be confidently predicted that no this person is not likely to have the disease. You can easily look and see that previous patients with similar blood sugar an age whom we actually tested are not diabetic. Thus we can conclude that this person is likely not to be diabetic either. Now recall we previously discussed how we can use lines to classify data among discrete classes well simply enough our data can be separated by a line.

This line is represented by some linear equation. How do we obtain the equation of this line. Well that's a much longer discussion and that we'll leave for later. But essentially there is a clear distinction that patients above the line are diabetic and patients below the line are not. That being said based on this line based on our predictive model which was obtained thanks to previous data sets we can effectively predict the probability of a new patient having diabetes. If for example someone has blood sugar levels or formula or supreme leader and is 55 years old this is a new datasets and it's not labeled. We haven't actually tested this person so we don't know.

We need to use our linear model which we obtained from previously labeled data sets to make a prediction. Clearly this co-ordinate is Plot and below the line. It can then be confidently assumed that this person is not likely to be diabetic. In summary what we're doing is first looking at previous data sets of patients who have already been labeled as having the disease or not. And based on that data we can come up with a pattern a predictive linear model a line that best separates our data into discrete classes classification in itself is a very important concept in self-driving cars. Implementing an algorithm is able to find this line with minimal error will be the entire focus of the section solving classification problems is essential in self-driving cars and can be used to classify whether an object crossing the road is a car a pedestrian a bicycle or even identifying different traffic signs.

This line is found based on previous datasets that have already been labeled as either healthy or sick this sets up a predictive model that can help us predict the labels of new unobserved data. In our case we developed a model that predicts the person's likeliness of being diabetic. And from that model depending on that person's age and blood sugar if the coordinate line is above the line then they're likely diabetic otherwise not. Let us now approach this a little bit more analytically to make a prediction.

First two inputs are processed. _**The first one is age which will be denoted as x 1 and the second one is the blood sugar which will be denoted as x 2. Accordingly the horizontal axis corresponds to the variable x 1 and you're probably used to the vertical axis being labeled as a y. We're going to label it x 2 so as to stay consistent and properly denote the second input variable.**_ Right now you and I can eyeball this data and dry a boundary line that separates the two regions considerably well eventually we'll implement an algorithm that's able to find this line on its own with a minimal error. Well let's assume that there are hundreds of iterations we're learning system that Terman that this line separates I read it into discrete classes. We can now use this data to classify new data as either healthy or sick depending on whether or not the data point falls below the line or above it the slides equation can be represented as x 2 is equal to negative 0.05 X1 plus 12. You're probably used to representing the equation of a line is y is equal to XP. Well this is the same exact thing as why is equal TMX supposed to be. That is where a Y would equal the 0.05 x 12. But it's important that we follow this notation instead so as to properly reference our input variables. What we'll do now is actually rearrange of this linear equation by bringing X to over to the other side whereas 0 = 0.05 X1 - x 2 + 12.

This is still the equation of our reline just to rearrange it. And the reason why we did this will be very clear momentarily. Let's take this equation and if you take any point that's above the line substitute to the linear equation where in this case X one is 80 and x 2 is 10. So being this and our the equation we get a negative number. Similarly if you take any point that's below the line we're in this case x 1 is 20 and x 2 was 5 7 into the equation and notice that you get a Howzat of number this linear model which we were able to develop based on previous data sets. Its equation can be used to determine the score if the person scores positive such that it's greater than or equal to zero. They likely are not diabetic. Otherwise if the person's score is negative they likely are diabetic.

Thus separating our data into two discrete classes. Let's recall a distinction between the label and the prediction. These data sets they've already been labeled as either red or blue. They have been given a correct output as having diabetes or not. The goal is to use these labeled datasets to train an algorithm to develop a linear model that can accurately classify this data. Initially the algorithms start with the random line with some random equation. Notice that X-1 and x 2 are multiplied by two constants. These two constants are called the weights which essentially dictate the slope of the line. And this year is known as the bias whose value dictates where the line intersects the vertical axis different weights and biases will result in different slopes and inclinations of our align as clearly shown in generally the equation for a linear model will be written as one X1 plus W 2 looks looks plus be what our algorithm will do is look at the initial data that's already been labeled. It will start with some random line to try and classify our training data alone like this one. Clearly this linear model is not a good one it misclassified a lot of the points its classification accuracy is not good and thus its error is very large. What our algorithm will do is it will keep adjusting the weights and biases of our linear model. Until it comes up with the optimized line that best classifies our data the line with minimal error our algorithm just made use of global datasets to train itself and come up with the perfect linear model. Let's assume this linear model has the following equation. We can now confidently use this model to predict the label of new data points. We do so by first determining the score. Suppose we have a person who has aged 40 years old with a blood sugar of two will then take the linear combination of these inputs which results in a score of eights. Now we can make a prediction. The prediction is noted as y hat which says that if that person with some age and some blood sugar if their score.

If the linear combination of their inputs is greater than or equal to zero if they are positive they are classified in Region 1. Otherwise if their score their linear combination is less than zero if it's negative they will be classified in region zero. In our case the score of a person aged 40 with the glucose levels of tumor the most supreme leader has a positive score and thus they can be categorized and costs one as not being diabetic which makes sense in the case of this line's equation since anything above the line will have a negative for anything below a positive score. The entire focus of the suction will be to train an algorithm to come up with a line whose predictions are accurate with data points that have already been labeled. That ends it must be able to correctly classify all the data points with maximum accuracy. There is one more concept we have to understand before we implement this into code and that is the concept of perceptrons.

The brain is a complex information processing device with incredible pattern recognition abilities it's able to freely process input from its surrounding environment categorize them and generate some kind of output. Similarly the perceptron is a basic processing element that also receives input processes it and generates an output based on the predictive model. More specifically those predictive linear model which was developed based on previously labeled data sets.

Let's place it inside of a node our model node is going to receive two input nodes the age and blood sugar x1 and x2 respectively. Suppose we want to predict if someone is diabetic the person is 30 years old has a blood sugar levels of three multi-month per liter. As such the input note X1 is 30 years old and the input node X2 is three the most per liter assume the line is the following equation.

The Perceptron on woman plot the point on the graph and check where that point is relative to the linear model based on this Lund's equation. If the person's linear combination is positive there are going to be plotted below the line and thus put a prediction of 1. Otherwise if the person's linear combination is negative they will be classified above the line and thus output a prediction of zero. In our case this person who is 30 years old with 3 million miles per liter of blood sugar has a positive linear combination is therefore going to be plotted somewhere below the line along with the other Blue

Points in Region 1 and thus we can safely predict that this person is in Class 1. They are not diabetic. Let's reshape our perception model and consider the bias as an input as well meaning we'll consider the person's age and the blood sugar and a bias value of one y one. Well if you really think about it what the first node in the perception does is it takes the inputs and multiplies each of them by their respective weights. The age X1 is multiplied by W 1 negative 0.1. The blood sugar x 2 is multiplied by W2 which would simply be one and the bias one is simply multiplied by the Bias value 14 and then taking the sum to obtain the score. There is actually a second node in the perceptron which we haven't talked about but we've been subconsciously using it all along and that is the activation function in neural networks the activation function of a node is what defines the outputs.

The most common type of activation function that we've been subconsciously using all along is the step function we've established that what the first node does is it takes the linear combination of all these inputs and now it's the step function which checks the result of our linear combination. If it's positive the point is below the line and therefore assign a label of 1. Otherwise if the score is less than zero The point is above the line and assign the label of zero. To recap all of this our input nodes whose linear combination is computed in the model node whose results then processed into the activation function which then predicts the outputs. All of this makes up our perception now that we know the basic node composition of a perception.

![](/assets/Screen Shot 2019-09-24 at 5.34.57 PM.png)The step function then checks the score and assigns that point a label of either 0 or 1. That is the basic composition of a perceptual and it takes an input data process that information in some way and finally the science how to categorize that data by producing and outputs. We also mentioned that when taking the linear combination of the inputs each input is multiplied by its respective weights. Looking at this equation age X1 is multiplied by a weight one and sugar level x 2 is multiplied by weight too.

Looking at this it should be clear that blood sugar has more weight than H. Suppose someone is 50 years old and has three million miles per liter of blood sugar. If we take their linear combination it results in a positive value of Six which our activation function would process and return a prediction of 1. If I change the age by a factor of 10 make it 60 years old it barely changes the value. Whereas if I change the sugar level by a factor of 10 and it has a dramatic impact on the score the input with the highest weight is the one that has the larger effect on the score. And clearly blood sugar has the higher effects its value is much more influential and pretty much dictates whether or not someone falls in class 1 or class 0 whereas age doesn't really have as much of an effect.

The goal is to use this data to train and develop an algorithm to come up with a line that best separates data into discrete classes with minimal error. As you know a line can be represented by an equation where w 1 and W to the weights dictate the slope of the line. These weights start out as the random values. And so we're just going to have a random line little more often than not is not going to classify our data correctly. Clearly this one does not. But as the neural network learns more about what kind of input did its dealing with it will adjust the weights based on the output errors that resulted in categorizing the data with the previous waits until it comes up with a good model. In essence through every iteration the network is being trained to come up with a better linear model.

And how does it do it. Pacifically How does it know how to adjust the weights instead of looking at 200 points. Let's simplify our example to 8 points the Blue Points are labeled 1 and the red points are also already labeled as 0. Now we need a line that separates this data into two classes. You and I can eyeball this and draw the following line that separates all of our data correctly. But the computer can eyeball it. It has no idea how to start. So what it's going to do is display a random line with some random equation. It's then going to look at this line and identify any errors in categorizing the falling datasets based on these errors. It's going to eventually adjust the weights of the linear your equation to fit the data better.

Where I essentially training the neural network. But how is it going to do that. Let's go back to our first line. Clearly these two points are misclassified. Suppose this is really in your equation. If you take the linear combination of each point each one would result in a negative score a value of less than zero. And I would step function would then predict each one the value is zero. But this is wrong since we initially assigned these points a label of 1. But our linear model predicts their value is zero. The key metric to solving this problem is the error which tells us if our model is doing well or not depending on how large or how small the error is.

And now through a gradient since our network is going to take tiny steps to reduce this error in every iteration these tiny steps correspond to the learning rate which needs to be sufficiently small since when that line is fixing itself you never want it to move drastically in one direction especially when you have a lot of points at every step. The line will move closer to them misclassified points and it keeps doing so until there are no errors or until the error is sufficiently small. And by doing so we eventually obtain the perfect linear model. Suppose the perfect linear model for the set of data was defined by the following equation. We cannot conclude that in classifying our data our linear model clearly gives more weight to X to rather the next one. The main theme of this lesson being the algorithm starts with a random line defined by random weights. The neural network as it calculates the errors it starts to learn more about the input data and then readjust the weights to minimize these errors and thus better classify our data. How does it calculate the air. How do we determine which points have the largest error relative to the current linear model.

How does it actually calculate the air.

Well we're going to need a continuous error function we'll call it.

Looking at this diagram clearly there are two misclassified points.

We know the Bluepoint need to be below the line and the red one above.

So it's going to happen is this error function is going to assign each misclassified point a big penalty.

As for the correctly classified points we're going to see very small penalties will have said that the

size of the points reflects the size of the penalty assigned to them.

The misclassified points have the largest penalties since they are misclassified.

And what we'll do is detect these error variations and thus figure out which direction we need to move

the line the most the total error and then results from some of these penalties associated with each

point.

We'll see if we assume that the total error is very high. So what we'll do is actually move the line in the direction with the most errors. We keep doing that until all error penalties are sufficiently small and thus we're minimizing the errors as we adjust the weight of our linear model to better classify the points and thereby minimizing and decreasing our total error some. Now what is this error function in order to answer this question and let's rethink our perception model. And the second node based on the score of each point predictive value of 0 or 1 any point with the positive score gets a 1 otherwise 0. These are discrete predictions which are derived from our step function.

The problem with this is being a step function increases or decreases very abruptly from one constant value to another. There is no in-between based on the value of the linear combination. The step function is discrete it's not continuous it's discrete in the sense that it only predicts values of either 0 or 1 instead of representing our predictions as discrete values of zeros and ones they need to be continuous probabilities. Which is why we cannot use the step function because Suppose we want it to predict the likeliness of someone who is let's say 62 years old with your blood sugar of 7.5 million miles per liter. We want to predict the likeliness of this person being diabetic.

The value of that person's linear combination based on this model is negative point one time 62 years old minus 7.5 plus 14. All of this equals 0.3. As expected it's a positive value since our point is below the line in the positive region. Our step function would simply check if that value is bigger or smaller than zero. In this case it is so predictive value of 1. This activation function doesn't actually return the likeliness of someone having diabetes or not. It's either a yes or no.

A very firm value. Well instead of a step function to determine the probability of someone being diabetic our activation function is going to be a sigmoid. There are many types of activation functions but we'll stick with the sigmoid which is represented by the following equation what that's going to do is take the value of our linear combination and subbing into the sigmoid function which returns a value of zero point five seven which means that according to our model this person who's 62 years old with blood sugar of 7.5 per liter is 57 percent likely to be healthy to not be diabetic. Let's say we test a person who's 20 years old with only two multi-multi per liter of blood sugar. There are linear combination would result in a value of 10. If we saw this value in our sigmoid one divided by one plus either the negative 10 results in the value of 99 percent which makes perfect sense. The younger someone is the more they tend to exercise.

And generally speaking would carry less weight as opposed to when they're older and the lower the blood sugar the more effective insulin is in storing sugar inside the cell the less diabetic risk. In other words the further you are from the line in the positive region the lower your blood sugar the younger you are and thus the more likely you are to be healthy the more you're in the positive region. And obviously the opposite occurs as you go into the negative region. As far as errors go with the sigmoid activation function Let's go back to this dataset and assume everyone who has already been tested and we've determined they are not diabetic they already have a label of one and never on and run has also been tested. We've determined they are diabetic. They are already labeled as 0. This is our training data. Our algorithm will try and classify this data and it will start with some random line. Looking at this data we can see that this point is misclassified if we know that the person isn't diabetic than his predicted probability of being healthy by the neural network should be higher than 50 percent.

However according to our linear model this person is in its negative region which is an error by adjusting both the weights and bias of our line. It then makes the correct prediction going forward will be using the sigmoid function as the activation function not the step function that is we'll proceed with the notion of what is the probability of someone being healthy or not instead of it being a discrete yes or no.

**Cross Entropy**

We then stopped and posed that ourselves with this question How does the computer specifically determine which linear model better classifies our data. This is where we shift our discussion over to cross entropy. The best model is the one that gives the higher probabilities for the points that we know should be in the positive region and the lowest probabilities to the points that we know should be in the negative. And we've already tested this person. They're labeled blue and thus they should be in the positive. But according to our linear model this point here is in the negative region since its probability is under 0.5 and anyone that to read. Suppose we've tested all of them and we know they're a diabetic yet this linear model predicts that this point is 80 percent in the positive 80 percent likely of it being healthy which is also wrong.

_**Clearly this model better classifies our data since it assigns the higher probability. So the point we know should be in the positive and the lowest probability to the points we know should be in the negative for the computer to determine that this model has a smaller error than this one. **\_Let's make use of cross entropy. _**What we need to do first is calculate the probability of the points being what they actually are. All of these probabilities correspond to the likeliness of something being blue of being in the positive. Further red points we need to determine their probabilities of them being red not blue. And since probabilities add up to one there are likeliness of being red of being in the negative is simply one minus the probability of it being blue on the left hand side one of our red points according to the linear model is only 20 percent likely of being red which doesn't make any sense**\_. We know this point is red. We already label it as such but according to the linear model it predicts that it's unlikely for this point to be red. All the red points are more likely to be red all the blue points are more likely to be blue. They are classified correctly.

This diagram you know has a much larger area than the second one.And given how we computed the probabilities the error can be written as the summation of the natural logarithm of each probability. Let's apply this equation to both our diagrams. We get a negative number for both which makes sense since the log of a number between 0 and 1 resulted in negative value. And we're just adding negatives aren't the negatives of what we'll do is reverse the sign by taking the negative of our resubmission. Looking at the results. It's clear that this model has a much larger error.

_**This summation of logarithms is what we'll refer to as cross entropy.**_ This is a very important concept when points are correctly classified. **They have small error penalties whereas when points are incorrectly classified they have big error penalties. **It follows that the linear model that better classifies the points is going to result in a low cross entropy and a bad model that incorrectly classifies its points has a large cross entropy value. This gives the same method of determining a good model from a bad model by simply calibrating the models cross entropy. Let's make this into an actual formula which we can then use in our code in the next video. The general equation for cross entropy is the following.

Where p is the probability of a point being blue and the variable y corresponds to a label of either 0 or 1. If our point is labeled blue if we tested this person and they are healthy it will have a label of 1. If our point is red Why Will equals zero. This makes sense of what we've done so far because think about it take this point for example it's blue. It has a label of 1 and a probability of zero point ninety five giving one times Law a point ninety five. And now the second term cancels though because one minus one is zero. And remember for the red points before putting them into the cross entropy equation we need to determine their probabilities of being red not blue. And since probabilities add up to 1 the probability of the Red Point being red is simply one minus P of blue. Which makes sense for this equation. Let's say computer for this point to the one that's 43 percent likely is being blue. And now since that red point would have a label of zero the first term cancels out and you're left with the one times the law and the probability of it being red. And if we keep doing that for every point notice we get the same result as before.

Except now this general equation that we arrived we can actually use it into our code. That's our official equation for cross entropy and by convention when computing the error function we consider the average of the sum which is why we divide by the total number of points m in the even if we were to take the average it's still apparent that the first linear model has a much better across century than the second one and therefore a much larger error. \_**This sets us up for grading the sense since now we're able to actually calculate the error with cross entropy. **\_Subsequently what brilliant descent will do is keep minimizing the error and obtain a linear model before moving on the gradient the scent in the next lesson will go back to our code.

**Gradient Descent**

The idea is that with some random displayed data the computer will display some random model based on that model it needs to calculate the error as we did previously. But now what we'll do is use gradient descent to minimize that error and obtain a better linear model which better classifies the data and we keep doing that over and over through many iterations until we obtain the perfect line to minimize the error. What we need to do is first take its gradient its derivative with respect to the weights. If we were to add the gradient to our linear parameters weight one weight 2 and the bias. What that will do will result in new weights and biases of a line with a much higher error function accordingly by subtracting that value. This tells us the value thats going to decrease the error function the most. Ultimately this results in a linear model with a smaller error that new linear model once again will calculate its error function subtract its gradient and keep doing that over and over minimizing the error function.

Eventually obtaining a line with small enough error that correctly classifies our data unfortunately in Python we're not able to simply take the derivative of the error function but have to actually derive the equation ourselves and then code it if you're interested in the math behind how the equation is derived. _**The gradient of the air function will simply be obtained by multiplying the coordinates of the points by the prediction minus the original label divided by the number of points subtracting the value from our linear parameters results in new updated weights for a new line which will thereby have a smaller error function than the previous.**_

One more thing we're going to do is we want to take very small leaps from one line to the other. We don't want the value of our lines to change drastically in each iteration. We know that subtracting the gradient adjust the weights of the line in the direction which gives us the least error but we don't want the line to move too far in that direction. So we'll scale I regret it down by multiplying it with what's called the learning rates which is normally a small value of let's say zero point zero one something small. This ensures that we're still going in the direction that best minimizes our error. But we're doing it in small steps and that's the formula for gradient the set.

**Multi-Classification - Softmask**

So right now we have effectively coded and discussed the theory behind the basic perceptual model and a more complex neural network. However both of these networks were similar in the sense that they were designed to separate a dataset containing only two labels 0 or 1. While the Perceptron and model dealt with linearly separable data were deep neural network dealt with deep separation of a more complex dataset. In this section will take this one step further by discussing multi-class datasets and the theory associated with these datasets will then implement this into code by separating our data into three classes. The term multi-class status simply refers to a database that contain more than two data classes. This means that three or more labels are present in the data.

Whereas before we had only dealt with data sets the had two labels 0 or 1.This introduces a brand new obstacles for data classification. The first crucial difference we face between dealing with binary data sets as opposed to dealing with multi-class data sets is the replacement of the sigmoid activation function. As discussed earlier the sigmoid function is a very useful tool to use for classifying binary data sets. It ranges in probability values between 0 and 1. The predicted probability of a point being zero as it infinitely approaches the negative region of a model and reaching close to 1 as it approaches positive infinity. And as you saw takes the form of the following equation which converts the variable x the score of a point to a probability.

These properties of the sigmoid activation function make it an ideal candidate to be used for classifying binary datasets. However it is not feasible to use this model for multi-class. It is a classification it spend them attention between 0 and 1 makes this difficult. So here we introduce a new activation function that is capable of dealing with multi-class data. _**That is the soft max function the max function is a useful activation function for scenarios involving multi-class functions and before we fully jump into the workings of the soft max function.**_

Let's take a look at an example of a multi-class dataset to better understand how these datasets are processed through our neural network imagine you are designing a neural network that is capable of distinguishing between different types of sports balls.

The networks should be able to distinguish between a soccer ball a basketball and a volleyball as there are three distinct classes to identify that can be seen that this is a multi-class data set. Now in the process of identifying these objects. The network must first take some inputs from the object that is to be identified. This can include things such as weight texture color and size. After these inputs are fed into the neural network they are manipulated by the weights and biased values of the connections between the input layer and the output there. Notice how there are three output letters. Since we're dealing with three mutually exclusive classifications we'll look more into that puts momentarily. But just think of how previously really dealt with a single output layer would simply return the probability of a point being in the positive region or not 0 or 1. _**However with multi-class classification. Suppose we passed in the puts x 1 x 2 x 3 next for which relate to weight texture color and size of some random ball and we fed it into our inner on that work through some feedforward operation this manipulation eventually returns some score value for each of the ball types since there are three outputs. We end up with three scores. So let's assume they're passing hour and puts throughout the entire add that to the neuron that work as the inputs are constantly being combined through the weights and biases of our network**_. Eventually the network outputs three scores such that the chances of our ball being a soccer ball get the score to the basketball gets a score of one. And finally the volleyball I get to score is zero. The score is nice to have I guess but I don't know but she did not really mean anything to me unless I'm able to convert these scores and the probabilities in order to make these values practical to use for prediction.

The first important thing to consider during this conversion is that the relative magnitudes of the scores must be maintained. This means that a score of two must have a higher probability than a score of one. And similarly So when comparing a score of one to a score is zero. That being said it's clear that by looking at the scores of the inputs of the random ball that we passed to our neural network most likely corresponds to that of a soccer ball. _**So the first big condition is that the relative magnitudes must be maintained. The second thing is to remember that all probability values must add up to one. Both of these requirements can be satisfied using the soft T-Max activation function.**_

That's exactly why for multi-class classification we must use the self-mocking function as it satisfies the condition. The soft max function is the following equation. _**This equation refers to the probability of some score and when there are any number of classes present in the data sets.**_

So what you would do is for each score and you to the end is divided by the summation of the exponential of all scores. Namely either the end divided by either the one plus either the two plus two to zero. This equation ensures that the relative magnitude of the scores is maintained and also ensures that all output probabilities sum to one as required. _**To get a better visual at this in our particular case if we sum the numbers into the soft max function taking the probability of our ball being a soccer ball so the score each of the two and divided by the exponential summation of all other scores either the two divided by either the two together the one with either the zero which results in the value of 0.67 we do the same thing for the probability of the point being a basketball the one divided by either of the two plus either the one plus you the zero which returns a point two for probability of the ball being basketball and finally doing the same thing for the volleyball results and the probability of 0.09.**_

_**Therefore the inputs the attributes of the ball we pass on to our neural network result in scores to 1 in 0 after being multiply and combined through various bias and values of our network. And then that's where we apply the softer max function. We apply it to our final layer which then determines probabilities of a 67 percent chance of the ball being a soccer ball 24 percent chance of the ball being a basketball and a 9 percent chance of it being in volleyball. Therefore our ball is more likely to be a soccer ball than anything else.**_

We have now found an appropriate activation function that deals well with the classification of multi-class data. The feedforward process to determine the probability of something being what it is is very similar to how we did it for binary classification or passing or input nodes through activation functions and every node of every hidden layer except now this time in the output layer we have to make use of the softer max function to convert our final scores to probabilities. We are able to make probability predictions on newly input data however recall it to make predictions on nearly and put a data first. We have to train the neural network.

And since we're dealing with a supervised learning algorithm we're training the neural network based on previously labeled data. We have to make use of datasets which already have discrete labels of being a soccer ball a basketball or a volleyball However there's just one problem. Our training data. How do we actually label it with binary classification. We simply had labels of either 0 or 1 and there were no issues of dependence among the two labels although transferring from binary datasets to multi-class data sets also presents another issue with multi-class data sets we cannot follow the same procedure. In the case of this example our intuition would tell us to label these classes from zero to two such as labeling the soccer balls to a basketball as the one in the volleyball is zero.

This seems like a concrete solution. However the issue with the solution is that it assumes dependency between the classes. This is what's known as label encodings which gives numerical aliases to different classes. The problem with this label encoding assumes some kind of relation. Some kind of dependence between our classes such that our algorithm might consider these to be in some kind of order 0 to 1 to 2. Which doesn't make any sense. These classes are mutually exclusive. We don't want the algorithm to consider one class more highly ordered than the other or process the classes with any sort of favoritism.

\_**For this reason we cannot use a label encoding to label our training that we use one hot encoding. One hot encoding allows us to classify all of our classes without assuming any dependence between the classes.One hot encoding works by creating separate columns for each day a label in the data set and using an appropriate one or zero value for each column to identify the data class for our data. **\_This would look like the following table since we're dealing with three classes. We create three columns. The presence of a class is presented in binary format. If something is a soccer ball it will be one hot uncoated as 1 0 0. If it's a basketball it to one hot and coded as 0 1 0. If it's a volley ball 0 0 1 as you can see this technique allows us to create independent labels for each of our data classes and ensures that our data does not assume any relation between our classes.

There are linearly independent. This can be done for any number of classes using the appropriate number of columns. Now we have gained another tool to use for classifying multi-class datasets using neural networks. All right so the process is similar to how we did it with binary classification or training a neural network based on previously labeled data which are one hot encoded using that label data to train our network to develop a perfect model and classifying it by updating the weights and biases of our a network through some gradient descent algorithm and then using that model we can make predictions on newly inputted data which don't have a label and predict their probability with the soft max function of it belonging to one of the three classes. That is the grand scheme of things.

Now that we have dealt with the logistics of data organization and data score analysis. Let's begin to discuss how the process of deep learning changes when we go from a binary data set to a multi-class dataset. We will discuss similar topics such as cross-bench or P. Gradient descent and back propagation in the context of multi-class datasets.

**Multi-Class Cross Entropy**

Cross-ownership is a very important topic that we discussed earlier in the context of a binary datasets. It is an important tool that allows us to distinguish between a good model and a bad one. Cross entropy is a method of measuring error with any neural network and consequently a lower cross-ownership revalue implies a more accurate system while the higher across and should be value implies a less accurate system. The previous form of that we discovered her cross entropy was of the following. This form really applies to a binary datasets where we have some number of points. Recall that ultimately what this equation did was take the law of the probability of the points being in the positive region which was then added to the line of the probability of the point being in the negative region. We did that for every single point then took the negative to gain the total cross entropy.

Since we've implemented it into our code anyways the larger the cross entropy value the larger the air the less accurate our model works and vice versa. Multi-class cross-bench we function the same way in the sense that I still provide the same tool for measuring the performance of our network. _**The same concept applies even if we're dealing with many classes the larger cross entropy the larger the error.**_

Let us refer back to our sports ball type example. To help explain just how multi-class across entropy is calculated. You first begin with a certain number of data points. For this example let's choose three arbitrary data points. Let's assume we have three unidentified balls each one with some input of size weight color and texture. What we'll do is pass the inputs of each ball into our neural network. First we pass in the inputs of the soccer ball and the neural network determines it has a 40 percent probability of it being a soccer ball. A 20 percent probability of it being in basketball any 40 percent chance of it being in volleyball which is a very accurate then repasts in the inputs of the basketball and the neural network and it has a 30 percent chance of being a soccer ball a 60 percent chance of being a basketball and a 10 percent chance of being in volleyball. This prediction is somewhat accurate And there are signs that the higher unlikeliness towards it being a basketball. And finally the volleyball passing and puts into the neural net where the neural network predicts its likeliness of it being 50 percent a soccer ball 30 percent of basketball and 20 percent of volleyball.

Let's applaud our results on the table. These are just tabulated values that the neural network predicted. Now ball one ball two involve three have already been labeled. They've already been one hot and as being a soccer ball basketball or volleyball. Although I a neural network the current model predicts that ball one only has a 40 percent chance of being a soccer ball ball to only as a 60 percent chance of being in basketball and ball 3. A 20 percent chance of being in volleyball. Clearly the current neural network is inaccurate. _**The model misclassified are added to whatever model we're using it has a large error we must calculate the total error and thus calculate the and should be for this specific set of data just like we did it for binary classification.**_

We simply take the natural logarithm and the probability of each disassembled being what it actually is our first ball which is already labeled as a soccer ball. It's probability of actually being a soccer ball according to our world that we're 0.4. So we'll take the loan of our second bottle already labeled as a basketball. It's predicted probability of actually being a basketball or according to our neural network is 0.6. So we'll take the loan of that and do the same thing for the labeled volleyball. _**We'll take the launch point to the launch of the prediction of it actually being available according to our model. We'll add everything up. And don't forget we always take the negative and thus obtained a cross entropy of three point four. And just like before the higher the cross and we value the higher the error. This is very similar to how we did it for binary classification except now instead of using binary across entropy we're making use of categorical Cross entropy should be this was our equation for binary cross-ownership entropy our categorical crosshatch equation is a little bit difference.**_

Having calculated the cross and before this set of data Well it's actually generalized categorical cross and into an actual formula like we did here for binary. What we'll do is first convert our probabilities into variables. The variables here represent the events the predictions made by the neural network on each ball being a soccer ball a basketball or a volleyball. In order to calculate a general formula for the cross entropy we must recognize the steps we took to calculate the cross and should be in our specific case above will represent the row as I and the columns. This makes sense if you think of this as a matrix with the rows and columns the axis traversing the rows is often denoted as I whereas the axes traversing the columns is often denoted as J anyways and the line of each prediction in all the cells in the first columns and multiply each row by the event of whether or not something is true. An event that's true will have a value of 1 while an event that does not occur will have a value of zero. So if we were to draw another table we'd end up with the following. The first ball is already labeled as a soccer ball. It's been one hot and coded as a soccer ball such that it has a value of 1 0 0. The first event is true. It does occur. We know this ball is a soccer ball. It's not a basketball or a volleyball. This one is not a soccer ball nor is it a volleyball. This one is a basketball so it's not encoded as 0 1 0 and the same concept applies to the volleyball. It's one hot and cold it is 001 to what we do is we take the last of each prediction multiply it by the event of it occurring are not giving us the cross entropy for a single ball. But notice the other two values cancel out. Which is exactly the point.

Thanks to the one hot encoding label it cancels the two cells and for the first ball takes the probability of the ball being what it actually yes. Then we do the same thing for the second column. Notice the first and third value cancels out. Thus only taking the line and the probability of the second ball being what it actually is a basketball and the same thing for the third column. The first and second volley cantaloupe and then we end up adding everything up along the axis.

The next step is to then take the negative and thus obtaining the total cross entropy. We know p 1 1 was initially point for p to 2 points 6. P 3 3 was initially point 2 and would you look at that. We get the same equation as before with the same result. Three point four and that will be the formula for multi-class cross entropy. As mentioned earlier the smaller the cross entropy value the better the model. So we then use some sort of gradient descent algorithm to minimize the error obtained. It is also important to mention that the process of gradient descent and the process of back propagation are both not generally effected by shifting the analysis of a binary dataset to a multi-class datasets.

We're still trying to minimize the error by updating our linear parameters based on the gradient of our error function. Nothing changed having discussed the main differences that must be considered when analyzing a binary dataset versus a multi-class data. 

